{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from prettytable import PrettyTable\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy.stats import mode\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, max_error\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_sub= pd.read_parquet('taxi/naive_submission.parquet')\n",
    "df= pd.read_parquet('taxi/train.parquet')\n",
    "test= pd.read_parquet('taxi/test.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(df.columns)\n",
    "# Variable qualitative ou quantitative\n",
    "quantitative_vars = ['passenger_count','trip_distance', 'PU_location_lat', 'PU_location_lon', 'DO_location_lat', 'DO_location_lon', 'fare_amount', 'tolls_amount', 'extra']\n",
    "qualitative_vars = ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'RatecodeID', 'store_and_fwd_flag', 'payment_type', 'mta_tax', 'improvement_surcharge', 'congestion_surcharge', 'Airport_fee']\n",
    "variable_cible = 'tip_amount'\n",
    "\n",
    "\n",
    "# Supprime observation nombre de passagers = 8 de nos données\n",
    "# Nous définissons un nouveau dataframe sur lequel nous faisons des transformations\n",
    "df_filtered = df.copy()\n",
    "index_8 = df_filtered[df_filtered['passenger_count']==7].index\n",
    "df_filtered = df_filtered.drop(index_8, axis=0)\n",
    "\n",
    "# Convertir en numérique les colonnes qui devraient l'être\n",
    "for col in quantitative_vars:\n",
    "    df_filtered[col] = pd.to_numeric(df_filtered[col], errors='coerce')\n",
    "\n",
    "for col in qualitative_vars:\n",
    "    df_filtered[col] = df_filtered[col].astype('category')\n",
    "\n",
    "# Variable binaire\n",
    "for col in ['store_and_fwd_flag','VendorID', 'mta_tax', 'congestion_surcharge', 'Airport_fee']:\n",
    "    df_filtered[col] = pd.factorize(df_filtered[col])[0] + 1\n",
    "\n",
    "# Variable improvement surcharge\n",
    "df_filtered['improvement_surcharge'] = pd.factorize(df_filtered['improvement_surcharge'])[0] + 1\n",
    "\n",
    "# RatecodeID en entiers\n",
    "df_filtered['RatecodeID'] = df_filtered['RatecodeID'].astype(int)\n",
    "\n",
    "# Convertir les autres colonnes en type \"category\" pour les colonnes catégorielles\n",
    "for col in qualitative_vars:\n",
    "    df_filtered[col] = df_filtered[col].astype('category')\n",
    "\n",
    "# Convertion en format data heure\n",
    "df_filtered['tpep_pickup_datetime'] = pd.to_datetime(df_filtered['tpep_pickup_datetime'])\n",
    "df_filtered['tpep_dropoff_datetime'] = pd.to_datetime(df_filtered['tpep_dropoff_datetime'])\n",
    "\n",
    "df_filtered = df_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering: À Lancer!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_of_day(hour):\n",
    "    if 4 <= hour < 12:\n",
    "        return 0\n",
    "    elif 12 <= hour < 18:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "# Clusterisation des quartiers/zones pour point de départ et arrivée\n",
    "# Appliquer KMeans pour 10 clusters\n",
    "kmeans_pickup = KMeans(n_clusters=10, random_state=42)\n",
    "kmeans_pickup.fit_predict(df_filtered[['PU_location_lat', 'PU_location_lon']])\n",
    "\n",
    "kmeans_dropoff = KMeans(n_clusters=10, random_state=42)\n",
    "kmeans_dropoff.fit_predict(df_filtered[['DO_location_lat', 'DO_location_lon']])\n",
    "\n",
    "# Transformation du nouveau dataframe\n",
    "def transform_df_fe(df, km_pickup, km_dropoff):\n",
    "    # Supprime cette observation de nos données\n",
    "    # Nous définissons un nouveau dataframe sur lequel nous faisons des transformations\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "    # Convertir en numérique les colonnes qui devraient l'être\n",
    "    for col in quantitative_vars:\n",
    "        df_filtered[col] = pd.to_numeric(df_filtered[col], errors='coerce')\n",
    "\n",
    "    # Variable catégorielles\n",
    "    for col in ['store_and_fwd_flag','VendorID', 'mta_tax', 'congestion_surcharge', 'Airport_fee', 'improvement_surcharge']:\n",
    "        to_map = np.unique(df_filtered[col])\n",
    "        df_filtered[col] = df_filtered[col].map({to_map[0]: 0, to_map[1]: 1})\n",
    "\n",
    "    # RatecodeID en entiers\n",
    "    df_filtered['RatecodeID'] = df_filtered['RatecodeID'].astype(int)\n",
    "\n",
    "    # Convertir les autres colonnes en type \"category\" pour les colonnes catégorielles\n",
    "    for col in qualitative_vars:\n",
    "        df_filtered[col] = df_filtered[col].astype('category')\n",
    "\n",
    "    # Feauter engineering\n",
    "    # Convertion en format data heure\n",
    "    df_filtered['tpep_pickup_datetime'] = pd.to_datetime(df_filtered['tpep_pickup_datetime'])\n",
    "    df_filtered['tpep_dropoff_datetime'] = pd.to_datetime(df_filtered['tpep_dropoff_datetime'])\n",
    "\n",
    "    df_fe = pd.DataFrame()\n",
    "    # Calculer le temps de trajet + nouvelle variable trajet moyen\n",
    "    df_fe['trip_duration'] = (df_filtered['tpep_dropoff_datetime'] - df_filtered['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "    # df_fe['average_speed'] = df_filtered['trip_distance']/df_fe['trip_duration']\n",
    "    df_fe['average_speed'] = np.where(df_fe['trip_duration'] == 0, 0, df_filtered['trip_distance'] / df_fe['trip_duration'])\n",
    "    \n",
    "    df_fe = df_fe.drop(['trip_duration'], axis=1)\n",
    "\n",
    "    # Time of day\n",
    "    df_fe['time_of_day'] = df_filtered['tpep_pickup_datetime'].dt.hour.apply(time_of_day)\n",
    "\n",
    "    # Creer une variable binaire indiquant si le trajet a eu lieu pendant le weekend (vendredi compris)\n",
    "    df_fe['is_weekend'] = df_filtered['tpep_pickup_datetime'].dt.dayofweek.apply(lambda x: True if x >= 5 else False)\n",
    "\n",
    "    df_fe['pickup_cluster'] = km_pickup.predict(df_filtered[['PU_location_lat', 'PU_location_lon']])\n",
    "    df_fe['dropoff_cluster'] = km_dropoff.predict(df_filtered[['DO_location_lat', 'DO_location_lon']])\n",
    "\n",
    "    # Frais supplementaire\n",
    "    df_fe[['extra', 'mta_tax', 'improvement_surcharge', 'congestion_surcharge', 'Airport_fee']] = df_filtered[['extra', 'mta_tax', 'improvement_surcharge', 'congestion_surcharge', 'Airport_fee']].apply(pd.to_numeric, errors='coerce')\n",
    "    df_fe['total_extra_fees'] = df_fe[['extra', 'mta_tax', 'improvement_surcharge', 'congestion_surcharge', 'Airport_fee']].sum(axis=1)\n",
    "    df_fe = df_fe.drop(['extra', 'mta_tax', 'improvement_surcharge', 'congestion_surcharge', 'Airport_fee'], axis=1)\n",
    "\n",
    "    # to category\n",
    "    for col in ['time_of_day', 'pickup_cluster', 'dropoff_cluster']:\n",
    "        df_fe[col] = df_fe[col].astype('category')\n",
    "\n",
    "    # Enlever les variables inutiles\n",
    "    to_remove = ['PU_location_lat', 'PU_location_lon', 'DO_location_lat', 'DO_location_lon', 'tpep_dropoff_datetime', 'tpep_pickup_datetime', 'mta_tax', 'improvement_surcharge', 'congestion_surcharge','store_and_fwd_flag']\n",
    "    df_filtered = df_filtered.drop(to_remove, axis=1)\n",
    "\n",
    "    df_filtered = df_filtered.join(df_fe)\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation des hyperparamètre pour une sélection de modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different models\n",
    "\n",
    "# Return the model with the best performance in cross-validation after grid search (hyperparameters that achieve best cv score)\n",
    "def tune_hyperparameters(model, method, param_grid, X_train, y_train, cv_folds, scoring):\n",
    "    gs_model = RandomizedSearchCV(model, param_grid, cv=cv_folds, scoring=scoring, n_jobs=-1, verbose=1)\n",
    "    gs_model.fit(X_train, y_train)\n",
    "    return gs_model.best_estimator_, gs_model.best_params_\n",
    "\n",
    "models = {\n",
    "    \"XGBoost\": XGBRegressor(enable_categorical=True),\n",
    "    \"CatBoost\": CatBoostRegressor(verbose=0, cat_features=['time_of_day', 'pickup_cluster', 'dropoff_cluster', 'VendorID', 'RatecodeID','Airport_fee']),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(),\n",
    "    \"LightGBM\": LGBMRegressor(verbose=0)\n",
    "}\n",
    "\n",
    "# Hyperparameters to compare during cv\n",
    "hyperparameter_grids = {\n",
    "    \"XGBoost\": {\n",
    "        'n_estimators': [100, 300, 500, 1000],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 6, 10, 15],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "        'reg_alpha': [0, 0.01, 0.1, 1],\n",
    "        'reg_lambda': [0.5, 1, 1.5, 2]\n",
    "    },\n",
    "    \n",
    "    \"CatBoost\": {\n",
    "        'iterations': [200, 500, 1000],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'depth': [4, 6, 8, 10],\n",
    "        'l2_leaf_reg': [1, 3, 5, 7],\n",
    "        'random_strength': [1, 2, 5, 10],\n",
    "        'bagging_temperature': [0, 1, 2, 5],\n",
    "        'border_count': [32, 64, 128]\n",
    "    },\n",
    "    \n",
    "    \"Random Forest Regressor\": {\n",
    "        'n_estimators': [100, 300, 500, 1000],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10, 15],\n",
    "        'min_samples_leaf': [1, 2, 4, 10],\n",
    "        'max_features': ['sqrt', 'log2', None],\n",
    "        'bootstrap': [True]\n",
    "    },\n",
    "    \n",
    "    \"Gradient Boosting\": {\n",
    "        'n_estimators': [100, 300, 500, 1000],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7, 10],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'min_samples_split': [2, 5, 10, 15],\n",
    "        'min_samples_leaf': [1, 2, 4, 10]\n",
    "    },\n",
    "    \n",
    "    \"LightGBM\": {\n",
    "        'n_estimators': [100, 300, 500, 1000],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'max_depth': [-1, 10, 20, 30],\n",
    "        'num_leaves': [31, 50, 100, 150],\n",
    "        'min_child_samples': [5, 10, 20, 30],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.7, 0.9, 1.0],\n",
    "        'reg_alpha': [0, 0.1, 1, 10],\n",
    "        'reg_lambda': [0.5, 1, 1.5, 2],\n",
    "        'verbose': [-1]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrainement uniquement sur les données avec méthode de paiement carte de crédit et avec un RatecodeID différent de 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Optimisation sur les donnees d'entrainement pour lequel payment_type=1 (carte de credit)\n",
    "df_train_fe = df_filtered[df_filtered['payment_type']==1]\n",
    "df_train_fe = df_train_fe[df_train_fe['RatecodeID']!=99]\n",
    "\n",
    "X_fe = df_train_fe.drop(['tip_amount'],axis=1)\n",
    "y_fe = df_train_fe['tip_amount']\n",
    "\n",
    "X_fe = transform_df_fe(X_fe, kmeans_pickup, kmeans_dropoff)\n",
    "\n",
    "X_fe = X_fe.drop('payment_type', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement: objectif MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "CatBoost\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Random Forest Regressor\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Gradient Boosting\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "LightGBM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    }
   ],
   "source": [
    "cv_folds = 5\n",
    "\n",
    "tuned_models = {}\n",
    "best_params = {}\n",
    "\n",
    "for method,model in models.items():\n",
    "    print(method)\n",
    "    tuned_model, params = tune_hyperparameters(model, method, hyperparameter_grids[method], X_fe, y_fe, cv_folds, scoring='neg_mean_squared_error')\n",
    "    tuned_models[method] = tuned_model\n",
    "    best_params[method] = params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement: objectif R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "CatBoost\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Random Forest Regressor\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Gradient Boosting\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "LightGBM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    }
   ],
   "source": [
    "cv_folds = 5\n",
    "\n",
    "tuned_models_r2 = {}\n",
    "best_params_r2 = {}\n",
    "\n",
    "for method,model in models.items():\n",
    "    print(method)\n",
    "    tuned_model, params = tune_hyperparameters(model, method, hyperparameter_grids[method], X_fe, y_fe, cv_folds, scoring='r2')\n",
    "    tuned_models_r2[method] = tuned_model\n",
    "    best_params_r2[method] = params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction d'ensemble par moyenne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tip_amount_ensemble(data, models):\n",
    "    # Initaliser un vecteur des predictions a 0\n",
    "    tip_predictions = np.zeros(len(data))\n",
    "    \n",
    "    # Predire uniquement les observations avec methode de paiment carte de credit\n",
    "    payment_type_1 = data[(data['payment_type'] == 1) & (data['RatecodeID'] != 99)]\n",
    "    payment_type_1 = payment_type_1.drop('payment_type', axis=1)\n",
    "\n",
    "    if not payment_type_1.empty:\n",
    "        predictions_ensemble = []\n",
    "\n",
    "        for method in models:\n",
    "            print(method)\n",
    "            pred = models[method].predict(payment_type_1)\n",
    "            predictions_ensemble.append(pred)\n",
    "\n",
    "        predictions_ensemble = np.array(predictions_ensemble).astype(float).T\n",
    "        mean_predictions = np.mean(predictions_ensemble, axis=1)\n",
    "\n",
    "        tip_predictions[(data['payment_type'] == 1) & (data['RatecodeID'] != 99)] = mean_predictions\n",
    "    return tip_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = transform_df_fe(test, kmeans_pickup, kmeans_dropoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "CatBoost\n",
      "Random Forest Regressor\n",
      "Gradient Boosting\n",
      "LightGBM\n",
      "XGBoost\n",
      "CatBoost\n",
      "Random Forest Regressor\n",
      "Gradient Boosting\n",
      "LightGBM\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_tip_amount_ensemble(new_test, tuned_models)\n",
    "predictions_r2 = predict_tip_amount_ensemble(new_test, tuned_models_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarder les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pd.DataFrame()\n",
    "# # results['Unnamed: 0'] = np.arange(len(test))\n",
    "# results['row_ID'] = np.arange(len(test))\n",
    "# results['tip_amount'] = predictions_r2\n",
    "# results\n",
    "# results.to_csv('results_taxi2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance sur les données d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "CatBoost\n",
      "Random Forest Regressor\n",
      "Gradient Boosting\n",
      "LightGBM\n",
      "Mean squared Error: 3.7392715988629317\n",
      "R2 score: 0.7248503274222702\n"
     ]
    }
   ],
   "source": [
    "new_train = transform_df_fe(df, kmeans_pickup, kmeans_dropoff)\n",
    "new_y = new_train['tip_amount']\n",
    "new_train = new_train.drop('tip_amount',axis=1)\n",
    "\n",
    "predictions_train = predict_tip_amount_ensemble(new_train, tuned_models)\n",
    "print('Mean squared Error:', mean_squared_error(new_y, predictions_train))\n",
    "print('R2 score:', r2_score(new_y, predictions_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvergarder les modèles pour utilisation ultérieure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_models(tuned_models, save_path=\"models/\"):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    for method, model in tuned_models.items():\n",
    "        # nom du fichier\n",
    "\n",
    "        filename = f\"{save_path}{method.replace(' ', '_')}_tuned_model.joblib\"\n",
    "        \n",
    "        # save modèle\n",
    "        joblib.dump(model, filename)\n",
    "\n",
    "        # Message\n",
    "        print(f\"Modèle '{method}' sauvegardé: '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle 'XGBoost' sauvegardé: 'models/XGBoost_tuned_model.joblib'\n",
      "Modèle 'CatBoost' sauvegardé: 'models/CatBoost_tuned_model.joblib'\n",
      "Modèle 'Random Forest Regressor' sauvegardé: 'models/Random_Forest_Regressor_tuned_model.joblib'\n",
      "Modèle 'Gradient Boosting' sauvegardé: 'models/Gradient_Boosting_tuned_model.joblib'\n",
      "Modèle 'LightGBM' sauvegardé: 'models/LightGBM_tuned_model.joblib'\n"
     ]
    }
   ],
   "source": [
    "save_all_models(tuned_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importer modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = joblib.load(\"tuned_model.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
